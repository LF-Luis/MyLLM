{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MyLLM Model architecture choice:\n",
    "- Token embedding layer\n",
    "- Transformer blocks\n",
    "    - Normalization layer\n",
    "    - Casual self-attention (using RoPE for positional embeddings)\n",
    "    - Dropout (following attention output)\n",
    "    - Residual with input to transformer block\n",
    "    - Normalization layer\n",
    "    - FFN\n",
    "        - Linear Layer (expands 4x the hidden size)\n",
    "        - SwiGLU\n",
    "        - Dropout (within FFN, after activation)\n",
    "        - Linear Layer (educes back to hidden size)\n",
    "    - Residual connection following previous residual output\n",
    "- Normalization Layer\n",
    "- Liner Layer (weight sharing with Token embedding layer)\n",
    "\n",
    "Pre-training dataset: 10BT (with gpt2 tokenizer, which is the same as r50k_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models with less than 1B parameters with similar architecture\n",
    "Following the advice of the [Deep Learning Tuning Playbook](https://github.com/google-research/tuning_playbook), let's look at the model and training parameters of similar models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### [Llama 3.2 (1B)](https://huggingface.co/meta-llama/Llama-3.2-1B)\n",
    "- Main diff is that it uses grouped-query attention, but that seems to be a memory-footprint optimization for larger models, so for the model here let's not worry about that.\n",
    "- It uses RoPE (though with more complex frequency setup), SwiGLU, and RMSNorm, same as MyLLM.\n",
    "- It does not use Dropout, as it expects its param size, training dataset size and other optimization to take care of generalization without overfitting.\n",
    "\n",
    "```python\n",
    "# Model Params\n",
    "hParams = HParams(\n",
    "    n_vocab = 128_256,  # Vocab size\n",
    "    n_ctx = 131_000,  # Token context length\n",
    "    n_embd = 2_048,  # Token embedding dimension\n",
    "    n_head = 32,  # Shares some heads\n",
    "    n_layer = 16,  # number of attention blocks\n",
    "    # ffn_act_pdrop -- no dropout\n",
    "    # attn_res_pdrop -- no dropout\n",
    ")\n",
    "hidden layer dimension = n_embd * 4 = 8_192.\n",
    "# Training Params\n",
    "Not much public info. 9 trillion tokens used in pre-training, knowledge distillation was used (from 3.1 models), ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3 (Small, Medium, Large)\n",
    "- [Paper](https://arxiv.org/pdf/2005.14165) and old [GPT-2 inference code](https://github.com/openai/gpt-2)\n",
    "- From a high-level, compared to MyLLM, GPT-3 doesn't use RoPE, SwiGLU, RMSNorm, or dropout.\n",
    "- Trained of 300B tokens.\n",
    "```python\n",
    "# Model Params\n",
    "hParams = \n",
    "HParams(       125M  |  350M  |  760M\n",
    "    n_vocab = 50_257   50_257   50_257\n",
    "    n_ctx =    2_048    2_048    2_048\n",
    "    n_embd =     768    1_024    1_536\n",
    "    n_head =      12       16       16\n",
    "    n_layer =     12       24       24\n",
    "    # ffn_act_pdrop -- no dropout\n",
    "    # attn_res_pdrop -- no dropout\n",
    ")\n",
    "hidden layer dimension = n_embd * 4\n",
    "# Training Params\n",
    "tParams = \n",
    "TParams(                125M  |  350M  |  760M  \n",
    "    token_batch_size =   0.5M    0.5M     0.5M\n",
    "    max_lr =             6e-4    3e-4   2.5e-4\n",
    "    Adam beta 1 =         0.9     0.9      0.9\n",
    "    Adam beta 2 =        0.95    0.95     0.95\n",
    "    epsilon =            1e-8    1e-8     1e-8\n",
    "    clip_grad_max_norm =  1.0     1.0      1.0  # clip global norm of the gradient at 1.0\n",
    "    weight_decay_rate =   0.1     0.1      0.1\n",
    ")\n",
    "AdamW Optimizer.\n",
    "Warmup + cosine decay LR scheduler:\n",
    "- There is a linear LR warmup over the first 375 million tokens, from ~0 to max_lr.\n",
    "- Then there's a cosine decay to 10% of the max_lr value (over 260 billion tokens).\n",
    "- Then training continues with 10% of max_lr.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLMo 1B\n",
    "- [Paper](https://arxiv.org/pdf/2402.00838) and [pre-training code](https://github.com/allenai/OLMo/blob/main/configs/official/OLMo-1B.yaml)\n",
    "- Like MyLLM, OLMo uses weight-tying, RoPE, SwiGLU. Doesn't use dropout or RMSNorm.\n",
    "- Trained on 2T tokens (using Dolma dataset, which has 2.668T tokens when these models were trained).\n",
    "```python\n",
    "# Model Params\n",
    "hParams = \n",
    "HParams(\n",
    "    n_vocab = 50_280\n",
    "    n_ctx =    2_048\n",
    "    n_embd =   2_048\n",
    "    n_head =      16\n",
    "    n_layer =     16\n",
    "    # ffn_act_pdrop -- no dropout\n",
    "    # attn_res_pdrop -- no dropout\n",
    ")\n",
    "hidden layer dimension = n_embd * 8 = 16_384\n",
    "# Training Params\n",
    "tParams = \n",
    "TParams(\n",
    "    token_batch_size =    ~4M\n",
    "    max_lr =             4e-4\n",
    "    alpha_f =             0.1\n",
    "    Adam beta 1 =         0.9\n",
    "    Adam beta 2 =        0.95\n",
    "    epsilon =            1e-5\n",
    "    clip_grad_max_norm =  1.0\n",
    "    weight_decay_rate =   0.1\n",
    ")\n",
    "AdamW Optimizer.\n",
    "Warmup + linear decay LR scheduler:\n",
    "- Linear warm up over ~21B tokens.\n",
    "- Then decay linearly to 10% max_lr, reaching that at the end of training.\n",
    "- Gradient clipping happens after warm-up period.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Foundry MPT (125M, 350M, 760M)\n",
    "- [Pre-Training code](https://github.com/mosaicml/llm-foundry/tree/main/scripts/train/yamls/pretrain)\n",
    "- Note that actual pre-train versions of these configs don't officially exist from Databricks, but it's still worth taking a look at.\n",
    "- MPT 7B was trained with 1T tokens.\n",
    "- Looking at the [pre-training code in HF for MPT 7B](https://huggingface.co/mosaicml/mpt-7b/tree/main), it seems to use RoPE and MHA. It's likely using GELU, LayerNorm, and weight-tying as well. (A deeper look at code + configs would be needed to be super sure as there's a lot going on in this code base.)\n",
    "\n",
    "```python\n",
    "# Model Params\n",
    "hParams = \n",
    "HParams(       125M  |  350M  |  760M\n",
    "    n_vocab = 50_368   50_368   50_257\n",
    "    n_ctx =    2_048    2_048    2_048\n",
    "    n_embd =     768    1_024    1_536\n",
    "    n_head =      12       16       16\n",
    "    n_layer =     12       24       24\n",
    "    # ffn_act_pdrop -- no dropout\n",
    "    # attn_res_pdrop -- no dropout\n",
    ")\n",
    "hidden layer dimension = n_embd * 4\n",
    "# Training Params\n",
    "tParams = \n",
    "TParams(                125M  |  350M  |  760M  \n",
    "    token_batch_size =  ~0.5M   ~0.5M    ~0.5M\n",
    "    max_lr =             6e-4    3e-4   2.5e-4\n",
    "    alpha_f =             0.1     0.1      0.1\n",
    "    Adam beta 1 =         0.9     0.9      0.9\n",
    "    Adam beta 2 =        0.95    0.95     0.95\n",
    "    epsilon =            1e-8    1e-8     1e-8\n",
    "    clip_grad_max_norm =  1.0     1.0      1.0\n",
    "    weight_decay_rate =   0.0     0.0      0.0\n",
    ")\n",
    "token_batch_size = (global_train_batch_size = 256) * (max_seq_len: 2048) = 524,288\n",
    "Decoupled AdamW Optimizer.\n",
    "Warmup + cosine decay LR scheduler:\n",
    "- There is a linear LR warmup over the first 100 batches, from ~0 to max_lr.\n",
    "- The cosine decay happens until end of training, where the last lr value will be `alpha_f * max_lr`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apple OpenELM (270M, 450M, 1.1B)\n",
    "- [Paper](https://arxiv.org/pdf/2404.14619)\n",
    "- [Model code](https://github.com/apple/corenet/blob/main/corenet/modeling/models/language_modeling/general_gpt.py)\n",
    "    - [Pre-Train Configs](https://github.com/apple/corenet/tree/2261885b6696950aaf481a862e8926921ef1a067/projects/openelm/pretraining_configs)\n",
    "- Unlike MyLLM, it uses larger LLama tokenizer, grouped query attention, layer-wise scaling.\n",
    "- Like MyLLM, it uses RoPE, SwiGLU, RMSNorm pre-normalization, weight-tying.\n",
    "- Trained on 1.5T tokens.\n",
    "\n",
    "```python\n",
    "# Model Params\n",
    "hParams = \n",
    "HParams(       270M  |  450M  |   1.1B\n",
    "    n_vocab = 32_128   32_128   32_128  # Actual is 32001, padded to make it hardware friendly 32_128 % (2^7) = 0\n",
    "    n_ctx =    2_048    2_048    2_048\n",
    "    n_embd =   1_280    1_526    2_048\n",
    "    n_head =      64       64       64  # num_gqa_groups=4,\n",
    "    n_layer =     16       20       28\n",
    "    # ffn_act_pdrop -- no dropout\n",
    "    # attn_res_pdrop -- no dropout\n",
    ")\n",
    "hidden layer dimension => n_embd * ffn_multipliers=(0.5, 4.0)  # Using layer-wise scaling\n",
    "# Training Params\n",
    "tParams = \n",
    "TParams(                270M  |  450M  |   1.1B\n",
    "    token_batch_size =    ~4M     ~4M      ~4M\n",
    "    max_lr =           0.0053  0.0039   0.0024\n",
    "    Adam beta 1 =         0.9     0.9      0.9\n",
    "    Adam beta 2 =        0.95    0.95     0.95\n",
    "    epsilon =            1e-8    1e-8     1e-8\n",
    "    clip_grad_max_norm =  1.0     1.0      1.0\n",
    "    weight_decay_rate =   0.1     0.1      0.1\n",
    ")\n",
    "AdamW Optimizer.\n",
    "Linear warmup + cosine decay LR scheduler:\n",
    "- Linear warmup from 0.000001 to max_lr.\n",
    "- Then it follows a cosine decay schedule, from max_lr to 10% of max_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of above configs of some open source models\n",
    "I'll use the following to help me pick training parameters for MyLLM. It's not the best way to do this, but it's a good start if you have limited GPU resources.  \n",
    "\n",
    "The MPT configs are grayed out because I couldn't find an official pre-trained model by them, it seems they just provided these configs. Also note that the MPT configs are similar to the GPT-3 configs, [varying mainly in the model architecture]\n",
    "![Summary table](../assets/some_open_source_models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_vocab\n",
    "n_ctx\n",
    "n_embd\n",
    "n_head\n",
    "n_layer\n",
    "Pos-Emb\n",
    "Attn \n",
    "FFN-Act\n",
    "FFN-Mult\n",
    "Norm\n",
    "Weight-tying  Yes\n",
    "\n",
    "Train Token Count  10BT\n",
    "token_batch_size  ~5M\n",
    "max_lr\n",
    "Adam_beta_1\n",
    "Adam_beta_2\n",
    "epsilon\n",
    "clip_grad_max_norm\n",
    "weight_decay_rate\n",
    "Optimizer\n",
    "LR schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use rough a back of the envelope calculation to figure how many tokens (and steps) to run linear warmup for. Let's first look at what other models are doing:\n",
    "\n",
    "(linear_ratio is the % of total training tokens that were processed during linear warm-up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_ratio: 0.00125, cos_dec_ratio: 0.8666666666666667\n",
      "remainder_ratio: 0.13208333333333333\n"
     ]
    }
   ],
   "source": [
    "# GPT-3 125M\n",
    "tot = 300e9  # Tokens\n",
    "linear = 375e6  # T\n",
    "cos_dec = 260e9 # T\n",
    "\n",
    "linear_ratio = linear / tot\n",
    "cos_dec_ratio = cos_dec / tot\n",
    "remainder_ratio = (tot - linear - cos_dec) / tot\n",
    "print(f\"linear_ratio: {linear_ratio}, cos_dec_ratio: {cos_dec_ratio}\")\n",
    "print(f'remainder_ratio: {remainder_ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schedule_ratios(model: str, tot: float, linear: float):\n",
    "    # Assuming all end with cosine decay\n",
    "    cos_dec = tot - linear\n",
    "    linear_ratio = linear / tot\n",
    "    cos_dec_ratio = cos_dec / tot\n",
    "    print(f\"{model:<11} | linear_ratio: {linear_ratio:.2e} | cos_dec_ratio: {cos_dec_ratio:.5}\") # {cos_dec_ratio:.2e}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 125   | linear_ratio: 1.25e-03 | cos_dec_ratio: 0.99875\n",
      "MPT 125     | linear_ratio: 5.20e-05 | cos_dec_ratio: 0.99995\n",
      "OpenELM 270 | linear_ratio: 1.33e-02 | cos_dec_ratio: 0.98667\n",
      "OLMo 1B     | linear_ratio: 1.05e-02 | cos_dec_ratio: 0.9895\n"
     ]
    }
   ],
   "source": [
    "get_schedule_ratios(\"GPT-3 125\", 300e9, 375e6)\n",
    "get_schedule_ratios(\"MPT 125\", 1e12, 52e6)\n",
    "get_schedule_ratios(\"OpenELM 270\", 1.5e12, 20e9)\n",
    "# get_schedule_ratios(\"GPT-3 350\", 300e9, 375e6)\n",
    "# get_schedule_ratios(\"MPT 350\", 1e12, 52e6)\n",
    "# get_schedule_ratios(\"OpenELM 450\", 1.5e12, 20e9)\n",
    "# get_schedule_ratios(\"GPT-3 760\", 300e9, 375e6)\n",
    "# get_schedule_ratios(\"MPT 760\", 1e12, 52e6)\n",
    "get_schedule_ratios(\"OLMo 1B\", 2e12, 21e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at GPT-3 125, OpenELM 270 (OpenELM 450 and 1.1B use the same numbers), and OLMo 1B, and ignoring MPT since it's too out of distribution and it was actually never trained, we get the following rough average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg: 8.35e-03\n",
      "Weighted avg: 9.77e-03\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"Avg: {(1.25e-03 + 1.33e-02 + 1.05e-02) / 3:.2e}\")\n",
    "\n",
    "numbers = [1.25e-03, 1.33e-02, 1.05e-02]\n",
    "pref = 0.4\n",
    "least_pref = 1.0 - (2 * pref)\n",
    "weights = [least_pref, pref, pref]\n",
    "\n",
    "print(f\"Weighted avg: {np.average(numbers, weights=weights):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking just at OpenELM and OLMo, since these are the most modern models in my list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19e-02\n"
     ]
    }
   ],
   "source": [
    "# Let's use the ratios from modern OpenELM and OLMo to set approximately\n",
    "# how many tokens will be processed in the linear warm-up\n",
    "avg = (1.33e-02 + 1.05e-02) / 2\n",
    "print(f\"{avg:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_warm_up_tokens: 1.190000e+08\n",
      "cos_decay_tokens: 9.881000e+09\n"
     ]
    }
   ],
   "source": [
    "batch_token_count = 524_288\n",
    "pre_train_tokens = 10e9  # ~10BT\n",
    "linear_warm_up_tokens = 1.19e-02 * pre_train_tokens\n",
    "cos_decay_tokens = pre_train_tokens - linear_warm_up_tokens\n",
    "print(f\"linear_warm_up_tokens: {linear_warm_up_tokens:e}\")\n",
    "print(f\"cos_decay_tokens: {cos_decay_tokens:e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm_up_steps: 226\n"
     ]
    }
   ],
   "source": [
    "warm_up_steps = int(linear_warm_up_tokens / batch_token_count)\n",
    "print(f\"warm_up_steps: {warm_up_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "226 steps for warm-up seems like a fine enough number to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the learning rate, there's no clear pattern from data I extracted above. GPT-3's learning rate may be too conservative, and OpenELM's and OLMo's may work best in setups where the batch size and training pool is very large. Maybe somewhere in middle is a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0021\n",
      "2.100e-03\n"
     ]
    }
   ],
   "source": [
    "# GPT-3 125 + OpenELM 270 + OLMo 1.1B\n",
    "avg = (6e-04 + 0.0053 + 4e-04) / 3\n",
    "print(f\"{avg:.3}\")\n",
    "print(f\"{avg:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.100e-03 may be a fine start.  \n",
    "Given our relatively small batch size, a smaller max learning rate is preferred to ensure stability and better generalization.  \n",
    "2.100e-03 is not as small as OLMo's 4e-04, but our linear increase + cosine decay schedule and gradient clipping should help with stability and positive learning (plus our use of dropout, weight decay, and normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0.0004  OLMo 1.1B\n",
    "0.0006  GPT-3 125\n",
    "0.0021  <--\n",
    "0.0053  OpenELM 270\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
